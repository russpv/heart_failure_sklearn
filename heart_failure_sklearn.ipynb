{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "4f64a7c4aa8c138f2a216c055f299fb9",
          "grade": false,
          "grade_id": "cell-757f7449dc485bc3",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "T7J5ewj8ISK-"
      },
      "source": [
        "# Heart Failure Prediction with Clinical Data\n",
        "\n",
        "## Overview\n",
        "\n",
        "Preparing the data, running basic statistics and building simple models are typical steps in a data science workflow. Here I demonstrate logistic binary classifiers, using clinical data as raw input to perform yes/no **Heart Failure Prediction**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "a33f4260efff941e320b24c95157b0e1",
          "grade": false,
          "grade_id": "cell-f4a83de52abb8394",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "nLcj3JrSISLB"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "luN9APEj8UmR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50d23991-12ba-4107-957c-1e0d4f6153b9"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T04:29:59.299540Z",
          "start_time": "2022-01-17T04:29:59.291328Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "17cd37dbc82c1a46157ec9adb81a3844",
          "grade": false,
          "grade_id": "cell-10db081f94b98d02",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "vgb9RbMBISLB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "DATA_PATH = \"/content/drive/MyDrive/HF1/HF1-lib/\"\n",
        "TRAIN_DATA_PATH = DATA_PATH + \"Train/\"\n",
        "VAL_DATA_PATH = DATA_PATH + \"Val/\"\n",
        "    \n",
        "sys.path.append(\"/content/drive/MyDrive/HF1/HF1-lib\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "a053ad27ddd1d1d271a6d28c51c634bc",
          "grade": false,
          "grade_id": "cell-52ddaebbf4a162a4",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "hJp489iOISLC"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "85cba0bf235d0a3f169d8bd9cf87350c",
          "grade": false,
          "grade_id": "cell-e59364a32d9b5fc9",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "0PSaVQ6cISLD"
      },
      "source": [
        "## 0 Raw Data\n",
        "\n",
        "The clinical dataset is synthesized from [MIMIC-III](https://www.nature.com/articles/sdata201635).\n",
        "\n",
        "3 CSV files are located in `TRAIN_DATA_PATH`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T04:29:59.429435Z",
          "start_time": "2022-01-17T04:29:59.302220Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7fdd12a66d23c25566d9167b74f1f34e",
          "grade": false,
          "grade_id": "cell-d92e9dc996c61070",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyDHxBmbISLD",
        "outputId": "fdc530c4-93b0-48e5-af20-3bf339826746"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "event_feature_map.csv  events.csv  hf_events.csv\n"
          ]
        }
      ],
      "source": [
        "!ls $TRAIN_DATA_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "46301c39a6360e38c7814cb9f4f1519a",
          "grade": false,
          "grade_id": "cell-d9212ce110d4dc0b",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "w-UZ4gPyISLD"
      },
      "source": [
        "**events.csv**\n",
        "\n",
        "These are patient event sequences. Each line is a tuple *(pid, event_id, vid, value)*. \n",
        "\n",
        "For example, \n",
        "\n",
        "```\n",
        "33,DIAG_244,0,1\n",
        "33,DIAG_414,0,1\n",
        "33,DIAG_427,0,1\n",
        "33,LAB_50971,0,1\n",
        "33,LAB_50931,0,1\n",
        "33,LAB_50812,1,1\n",
        "33,DIAG_425,1,1\n",
        "33,DIAG_427,1,1\n",
        "33,DRUG_0,1,1\n",
        "33,DRUG_3,1,1\n",
        "```\n",
        "\n",
        "- **pid**: De-identified patient identier. For example, the patient in the example above has pid 33. \n",
        "- **event_id**: Clinical event identifier. For example, DIAG_244 means the patient was diagnosed of disease with ICD9 code [244](http://www.icd9data.com/2013/Volume1/240-279/240-246/244/244.htm); LAB_50971 means that the laboratory test with code 50971 was conducted on the patient; and DRUG_0 means that a drug with code 0 was prescribed to the patient. Corresponding lab (drug) names are in `{DATA_PATH}/lab_list.txt` (`{DATA_PATH}/drug_list.txt`).\n",
        "- **vid**: Ordinal visit identifier for patient visits to the healthcare provider.\n",
        "- **value**: Redundant, binary, signifying associated event occurred (always 1 in the synthesized dataset)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "dd71df788c3b463c5db5d09b8991575d",
          "grade": false,
          "grade_id": "cell-b20f2e82052a2926",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "WEt8rNYZISLE"
      },
      "source": [
        "**hf_events.csv**\n",
        "\n",
        "Contains id code of patients who have been diagnosed with heart failure; tuples of *(pid, vid, label)*. As in, only patients with heart failure in present, and this needs to be mixed with patients not diagnosed with heart failure. For example,\n",
        "\n",
        "```\n",
        "156,0,1\n",
        "181,1,1\n",
        "```\n",
        "\n",
        "The vid indicates the index of the first visit with heart failure of that patient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "c5d154237645fcb55795c002c2c56c9f",
          "grade": false,
          "grade_id": "cell-8a8108bebe9e0d39",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "akNIV3qfISLF"
      },
      "source": [
        "**event_feature_map.csv**\n",
        "\n",
        "The *event_feature_map.csv* is a map from an event_id to an integer index. This file contains *(idx, event_id)* pairs for all event ids."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "4cac1012cd2d5bacf94ba5393cf3ac3b",
          "grade": false,
          "grade_id": "cell-e092836975c6797a",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "ebY89zr4ISLF"
      },
      "source": [
        "## 1 Statistics\n",
        "\n",
        "Each line represents an event (of which there can be multiple) that happened at an encounter.\n",
        "\n",
        "- **Event count**: Number of events recorded for a given patient.\n",
        "- **Encounter count**: Number of visits recorded for a given patient.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T04:29:59.718385Z",
          "start_time": "2022-01-17T04:29:59.432216Z"
        },
        "deletable": false,
        "id": "u5dCLbDpISLG"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "\n",
        "def read_csv(filepath=TRAIN_DATA_PATH):\n",
        "    '''\n",
        "    Reads the events.csv and hf_events.csv files. \n",
        "    Variables returned from this function are passed as input to the metric functions.\n",
        "    '''\n",
        "    \n",
        "    events = pd.read_csv(filepath + 'events.csv')\n",
        "    hf = pd.read_csv(filepath + 'hf_events.csv')\n",
        "\n",
        "    return events, hf\n",
        "\n",
        "def event_count_metrics(events, hf):\n",
        "    '''\n",
        "    Returns event count metrics.\n",
        "    Event count is defined as the number of events recorded for a given patient.\n",
        "    '''\n",
        "\n",
        "    avg_hf_event_count = None\n",
        "    max_hf_event_count = None\n",
        "    min_hf_event_count = None\n",
        "    avg_norm_event_count = None\n",
        "    max_norm_event_count = None\n",
        "    min_norm_event_count = None\n",
        "    \n",
        "    df1 = hf_events_table(events, hf)\n",
        "    avg_hf_event_count = df1['eventcount_fromevents'].mean()\n",
        "    max_hf_event_count = df1['eventcount_fromevents'].max()\n",
        "    min_hf_event_count = df1['eventcount_fromevents'].min()\n",
        "    \n",
        "    df2 = norm_events_table(events, hf)\n",
        "    avg_norm_event_count = df2['eventcount_fromevents'].mean()\n",
        "    max_norm_event_count = df2['eventcount_fromevents'].max()\n",
        "    min_norm_event_count = df2['eventcount_fromevents'].min()\n",
        "\n",
        "    return avg_hf_event_count, max_hf_event_count, min_hf_event_count, \\\n",
        "           avg_norm_event_count, max_norm_event_count, min_norm_event_count\n",
        "\n",
        "def encounter_count_metrics(events, hf):\n",
        "    '''\n",
        "    Returns encounter count metrics.\n",
        "    Encounter count is defined as the number of visits recorded for a given patient. \n",
        "    '''\n",
        "\n",
        "    avg_hf_encounter_count = None\n",
        "    max_hf_encounter_count = None\n",
        "    min_hf_encounter_count = None\n",
        "    avg_norm_encounter_count = None\n",
        "    max_norm_encounter_count = None\n",
        "    min_norm_encounter_count = None\n",
        "    \n",
        "    df1 = hf_visit_table(events, hf) \n",
        "    avg_hf_encounter_count = df1['visitcount_fromevents'].mean()\n",
        "    max_hf_encounter_count = df1['visitcount_fromevents'].max()\n",
        "    min_hf_encounter_count = df1['visitcount_fromevents'].min()\n",
        "    \n",
        "    df2 = norm_visit_table(events, hf) \n",
        "    avg_norm_encounter_count = df2['visitcount_fromevents'].mean()\n",
        "    max_norm_encounter_count = df2['visitcount_fromevents'].max()\n",
        "    min_norm_encounter_count = df2['visitcount_fromevents'].min()\n",
        "\n",
        "    return avg_hf_encounter_count, max_hf_encounter_count, min_hf_encounter_count, \\\n",
        "           avg_norm_encounter_count, max_norm_encounter_count, min_norm_encounter_count\n",
        "\n",
        "\n",
        "def left_anti(events, hf):\n",
        "    '''\n",
        "    This gives full data on the 1040 non-hf patients\n",
        "    '''\n",
        "    df = pd.merge(left=events,\n",
        "                  right=hf, \n",
        "                  on ='pid', \n",
        "                  how ='left',\n",
        "                  indicator=True,\n",
        "                  suffixes=('_fromevents', '_fromhf')) \\\n",
        "                  .query('_merge == \"left_only\"') \\\n",
        "                  .drop('_merge', 1)\n",
        "    return df\n",
        "\n",
        "def left_join_pid(events, hf):\n",
        "    '''\n",
        "    This gives full data on the 4000 patients, with the hf diagnosis visit id on the 2060 patients\n",
        "    '''\n",
        "    df = pd.merge(left=events, \n",
        "                right=hf, \n",
        "                on ='pid', \n",
        "                how ='left',\n",
        "                suffixes=('_fromevents', '_fromhf'))\n",
        "    return df\n",
        "\n",
        "def right_join_pid(events, hf):\n",
        "    '''\n",
        "    This gives full data on the 2060 hf patients\n",
        "    '''\n",
        "    df = pd.merge(left=events, \n",
        "                right=hf, \n",
        "                on ='pid', \n",
        "                how ='right',\n",
        "                suffixes=('_fromevents', '_fromhf'))\n",
        "    return df\n",
        "\n",
        "def hf_events_table(events, hf):\n",
        "    '''\n",
        "    Returns (non-unique) event counts in 'event_id' column\n",
        "    '''\n",
        "    df = right_join_pid(events, hf)\n",
        "    groupby1 = df.groupby([\"pid\"], as_index=False).count() \\\n",
        "      .rename(columns={'event_id':'eventcount_fromevents'})\n",
        "    return groupby1\n",
        "\n",
        "def norm_events_table(events, hf):\n",
        "    '''\n",
        "    Returns (non-unique) event counts in 'event_id' column\n",
        "    '''\n",
        "    df = left_anti(events, hf)\n",
        "    groupby3 = df.groupby([\"pid\"], as_index=False).count() \\\n",
        "      .rename(columns={'event_id':'eventcount_fromevents'})\n",
        "    return groupby3\n",
        "\n",
        "def hf_visit_table(events, hf):\n",
        "    '''\n",
        "    Returns unique visit counts in 'vid_fromevents' column, NOT vids\n",
        "    THIS MAY BE A PROBLEM IF THE VIDS SKIP NUMBERS, I'M CORRECTING THIS HERE\n",
        "    '''\n",
        "    df = right_join_pid(events, hf)\n",
        "    # group on unique visits and aggregate-count\n",
        "    groupby1 = df.groupby([\"pid\", \"vid_fromevents\"], as_index=False).count() \\\n",
        "      .rename(columns={'vid_fromevents':'visitcount_fromevents'}) \\\n",
        "      .drop(columns=['value', 'event_id', 'vid_fromhf', 'label'])\n",
        "    # group on patient for unique visit counts\n",
        "    groupby2 = groupby1.groupby([\"pid\"]).count()\n",
        "    return groupby2\n",
        "\n",
        "def norm_visit_table(events, hf):\n",
        "    '''\n",
        "    Returns unique visit counts in 'vid_fromevents' column, NOT vids\n",
        "    '''\n",
        "    df = left_anti(events, hf)\n",
        "    groupby3 = df.groupby([\"pid\", \"vid_fromevents\"], as_index=False).count() \\\n",
        "      .rename(columns={'vid_fromevents':'visitcount_fromevents'}) \\\n",
        "      .drop(columns=['value', 'event_id', 'vid_fromhf', 'label'])\n",
        "    groupby4 = groupby3.groupby([\"pid\"]).count()\n",
        "    return groupby4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### FINDINGS\n",
        "## There are 4000 patients in total\n",
        "## There are 2960 patients with hf\n",
        "## 1040 patients without hf\n",
        "## There is 100% overlap of hf on events\n",
        "# events, hf = read_csv(TRAIN_DATA_PATH)\n",
        "# norm_visit_table(events, hf)"
      ],
      "metadata": {
        "id": "uO7S6swe8sw8"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T04:30:00.227147Z",
          "start_time": "2022-01-17T04:29:59.720539Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9e7427796cf3b4d5a493bd096d082ac9",
          "grade": false,
          "grade_id": "cell-d28045e592b8f081",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "B0G6ViLRISLG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22a4d0c7-b924-4642-e27d-6159cfb643b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:82: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time to compute event count metrics: 0.4409446716308594s\n",
            "(188.9375, 2046, 28, 118.64423076923077, 1014, 6)\n",
            "Time to compute encounter count metrics: 0.44466209411621094s\n",
            "(2.8060810810810812, 34, 2, 2.189423076923077, 11, 1)\n"
          ]
        }
      ],
      "source": [
        "if __name__=='__main__':\n",
        "\n",
        "  events, hf = read_csv(TRAIN_DATA_PATH)\n",
        "\n",
        "  #Compute the event count metrics\n",
        "  start_time = time.time()\n",
        "  event_count = event_count_metrics(events, hf)\n",
        "  end_time = time.time()\n",
        "  print((\"Time to compute event count metrics: \" + str(end_time - start_time) + \"s\"))\n",
        "  print(event_count)\n",
        "\n",
        "  #Compute the encounter count metrics\n",
        "  start_time = time.time()\n",
        "  encounter_count = encounter_count_metrics(events, hf)\n",
        "  end_time = time.time()\n",
        "  print((\"Time to compute encounter count metrics: \" + str(end_time - start_time) + \"s\"))\n",
        "  print(encounter_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "d67dbea2960c373fa33683228f344817",
          "grade": false,
          "grade_id": "cell-c5413d29c37a0a33",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "2uqq7RE9ISLH"
      },
      "source": [
        "## 2 Feature construction\n",
        "\n",
        "Here I convert raw data into a SVM Light format for sparse dimension data.\n",
        "\n",
        "I create some important objects:\n",
        "\n",
        "- **Index vid**: Index vid:\n",
        "  - For heart failure patients: Index vid is the vid of the first visit with heart failure for that patient (i.e., vid field in *hf_events.csv*). \n",
        "  - For normal patients: Index vid is merely the vid of the last visit for that patient (i.e., vid field in *events.csv*). \n",
        "- **Observation Window**: The time interval of modeling interest per patient.\n",
        "  - For heart failures: the visits prior to the visit with HF diagnosis (we don't care about what happens after HF is already known)\n",
        "  - For normals: \n",
        "- **Prediction Window**: The point where a HF prediction will be made.\n",
        "\n",
        "If index vid is 3, visits with vid 0, 1, 2 are within the observation window, and the prediction window is between visit 2 and 3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "e0a8860140e64060ac50e4ca3c701ea1",
          "grade": false,
          "grade_id": "cell-aa768bbdeed84907",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "h4bTH9IUISLI"
      },
      "source": [
        "### 2.1 Index vid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T04:30:01.160227Z",
          "start_time": "2022-01-17T04:30:01.154297Z"
        },
        "deletable": false,
        "id": "3ENVVXUbISLI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import datetime\n",
        "\n",
        "\n",
        "def read_csv(filepath=TRAIN_DATA_PATH):\n",
        "    events = pd.read_csv(filepath + 'events.csv')\n",
        "    hf = pd.read_csv(filepath + 'hf_events.csv')\n",
        "    feature_map = pd.read_csv(filepath + 'event_feature_map.csv')\n",
        "\n",
        "    return events, hf, feature_map\n",
        "\n",
        "\n",
        "def calculate_index_vid(events, hf):\n",
        "    '''\n",
        "    Returns pandas dataframe with header `['pid', 'indx_vid']`.\n",
        "\n",
        "    Suggested steps:\n",
        "        1. Create list of normal patients (hf_events.csv only contains information about heart failure patients).\n",
        "        2. Split events into two groups based on whether the patient has heart failure or not.\n",
        "        3. Calculate index vid for each patient.\n",
        "    '''\n",
        "\n",
        "    indx_vid = ''\n",
        "    \n",
        "    # 2960 hfs: get last visit from hf table as indx_vid (max is unnecessary)\n",
        "    df_hf_vid_max = right_join_pid(events, hf).groupby(['pid'], as_index=True).max()\n",
        "    # set the index visit as the vid from hf\n",
        "    df_hf_vid_max['indx_vid'] = df_hf_vid_max['vid_fromhf']\n",
        "\n",
        "    # 1040 normies: get last visit from norm table as indx_vid\n",
        "    '''\n",
        "    In reviewing my code, I identify this point as important choice\n",
        "    MAJOR SOURCE OF CONFUSION: CHOICE BETW (TOTAL VISIT COUNT - 1) OR ORIG VID\n",
        "    --------------------------------------------------------------------------\n",
        "    '''\n",
        "    choice = 'orig vid'\n",
        "    if choice == 'orig vid':\n",
        "      # Orig vid approach\n",
        "      df_norm_vid_max = left_anti(events, hf).groupby(['pid'], as_index=True).max()\n",
        "      df_norm_vid_max['indx_vid'] = df_norm_vid_max['vid_fromevents']\n",
        "    elif choice == 'visitcount':\n",
        "      # Total visit count approach\n",
        "      df_norm_vid_max = norm_visit_table(events, hf).groupby(['pid'], as_index=True).max()\n",
        "      df_norm_vid_max['indx_vid'] = df_norm_vid_max['visitcount_fromevents'] - 1\n",
        "    else:\n",
        "      raise ValueError('choice variable set incorrectly')\n",
        "\n",
        "    indx_vid = df_hf_vid_max.append(df_norm_vid_max).reset_index().drop(columns=['event_id','vid_fromevents','value','vid_fromhf','label'])\n",
        "\n",
        "    return indx_vid"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### TEST CELL\n",
        "## There are 4000 patients in total\n",
        "## There are 2960 patients with hf\n",
        "## 1040 patients without hf\n",
        "## There is 100% overlap of hf on events\n",
        "\n",
        "events, hf, _ = read_csv(TRAIN_DATA_PATH)\n",
        "df = calculate_index_vid(events, hf)\n",
        "df[df.pid == 78]"
      ],
      "metadata": {
        "id": "nA_Fr55aEJlJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "cc6a5208-ea74-4749-e10b-ad501ed1f8c8"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:82: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-738801d4-8150-4133-822c-3ec568b7ee2a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pid</th>\n",
              "      <th>indx_vid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2960</th>\n",
              "      <td>78</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-738801d4-8150-4133-822c-3ec568b7ee2a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-738801d4-8150-4133-822c-3ec568b7ee2a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-738801d4-8150-4133-822c-3ec568b7ee2a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      pid  indx_vid\n",
              "2960   78         1"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "c2bba556d9b1940fed3518e2f0b90e05",
          "grade": false,
          "grade_id": "cell-4a3b0c07c74dc8dc",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "C7KzE0C5ISLI"
      },
      "source": [
        "### 2.2 Filter events\n",
        "\n",
        "This removes events outside the observation window."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T04:30:01.469360Z",
          "start_time": "2022-01-17T04:30:01.465690Z"
        },
        "deletable": false,
        "id": "jV13ZKndISLI"
      },
      "outputs": [],
      "source": [
        "def filter_events(events, indx_vid):\n",
        "    '''\n",
        "    Returns dataframe of the observation window data for HF patients\n",
        "    `['pid', 'event_id', 'value']` up to but not including the index visit.\n",
        "\n",
        "      1. Join indx_vid with events on pid\n",
        "      2. Filter events\n",
        "    \n",
        "    indx_vid columns = index, pid, indx_vid\n",
        "    events columns = pid, event_id, vid, value\n",
        "    '''\n",
        "\n",
        "    filtered_events = None\n",
        "    \n",
        "    '''\n",
        "    ---------------------------\n",
        "    Key choice at this point:\n",
        "    FILTER (inner join) BY INDX_VID\n",
        "    ---------------------------\n",
        "    '''\n",
        "    # join dfs on two columns\n",
        "    inner_join = pd.merge(left=indx_vid, \n",
        "            right=events, \n",
        "            on=['pid'], \n",
        "            how ='inner')\n",
        "    \n",
        "    # filter all event_ids less than idx_vid\n",
        "    filtered_events = inner_join[(inner_join.vid < inner_join.indx_vid)]\n",
        "\n",
        "    return filtered_events.drop(columns=['indx_vid', 'vid'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "### TEST\n",
        "events, hf, feature_map = read_csv(TRAIN_DATA_PATH)\n",
        "index = calculate_index_vid(events, hf)\n",
        "# left_outer = pd.merge(left=indx_vid, \n",
        "#         right=events, \n",
        "#         left_on=['pid'], \n",
        "#         right_on=['pid'],\n",
        "#         how ='left',\n",
        "#         suffixes=('_fromidx', '_fromevents'))\n",
        "# df = left_outer[(left_outer.vid < left_outer.indx_vid)]\n",
        "# df[df.pid == 78]\n",
        "filter_events(events, index)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "q-IKGO09Pvtt",
        "outputId": "f9818ace-6b75-45d6-e0f2-c9ee573e5612"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n### TEST\\nevents, hf, feature_map = read_csv(TRAIN_DATA_PATH)\\nindex = calculate_index_vid(events, hf)\\n# left_outer = pd.merge(left=indx_vid, \\n#         right=events, \\n#         left_on=['pid'], \\n#         right_on=['pid'],\\n#         how ='left',\\n#         suffixes=('_fromidx', '_fromevents'))\\n# df = left_outer[(left_outer.vid < left_outer.indx_vid)]\\n# df[df.pid == 78]\\nfilter_events(events, index)\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "a74bda5c299d66e1901820f18bb9aa25",
          "grade": false,
          "grade_id": "cell-845f881f710c6b3b",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "LlsButlqISLJ"
      },
      "source": [
        "### 2.3 Aggregate events\n",
        "\n",
        "I will use count of events as a feature, and so prepare it here:\n",
        "\n",
        "So for this patient...\n",
        "\n",
        "```\n",
        "33,DIAG_244,0,1\n",
        "33,LAB_50971,0,1\n",
        "33,LAB_50931,0,1\n",
        "33,LAB_50931,0,1\n",
        "33,DIAG_244,1,1\n",
        "33,DIAG_427,1,1\n",
        "33,DRUG_0,1,1\n",
        "33,DRUG_3,1,1\n",
        "33,DRUG_3,1,1\n",
        "```\n",
        "...will become *(event_id, value)* \n",
        "\n",
        "```\n",
        "(DIAG_244, 2.0)\n",
        "(LAB_50971, 1.0)\n",
        "(LAB_50931, 2.0)\n",
        "(DIAG_427, 1.0)\n",
        "(DRUG_0, 1.0)\n",
        "(DRUG_3, 2.0)\n",
        "```\n",
        "\n",
        "Then index the event codes using *event_feature_map.csv*.\n",
        "\n",
        "```\n",
        "(146, 2.0)\n",
        "(1434, 1.0)\n",
        "(1429, 2.0)\n",
        "(304, 1.0)\n",
        "(898, 1.0)\n",
        "(1119, 2.0)\n",
        "```\n",
        "\n",
        "Also, I min-max normalize to work in 0-1 space. For simplicity I define min(x) as zero, so just $x$/$max(x)$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T04:30:02.033863Z",
          "start_time": "2022-01-17T04:30:02.028268Z"
        },
        "deletable": false,
        "id": "7lSKXAkdISLJ"
      },
      "outputs": [],
      "source": [
        "def aggregate_events(filtered_events_df, _, feature_map_df):\n",
        "    '''\n",
        "    Returns dataframe with header `['pid', 'feature_id', 'feature_value']`\n",
        "\n",
        "      1. Lookup event_id's from event_feature_map.csv.\n",
        "      2. Aggregate events with count\n",
        "      3. Normalize with min-max (the min value is assumed zero).\n",
        "\n",
        "    filtered_events_df > 'pid', 'event_id', 'value'\n",
        "    feature_map_df > 'idx' , 'event_id'\n",
        "    '''\n",
        "    \n",
        "    aggregated_events = None\n",
        "  \n",
        "    ### Step 0: remap eventids to integers\n",
        "    feature_map_df = feature_map_df.rename(columns={'idx':'feature_id'})\n",
        "\n",
        "    inner_join = pd.merge(left=filtered_events_df, \n",
        "        right=feature_map_df, \n",
        "        on=['event_id'], \n",
        "        how ='inner').drop(columns=['value'])\n",
        "\n",
        "    ### Step 1: get the per-patient frequency of (patient-)events\n",
        "    inner_join['freq'] = 1\n",
        "    '''\n",
        "    NOTE: Here I decide to keep duplicate events per visit. But I would probably \n",
        "    remove duplicates if using this model further.\n",
        "    --------------------------------------------------------------------------\n",
        "    '''\n",
        "    counts = inner_join.groupby(['pid', 'feature_id'], as_index=True).count() \\\n",
        "      .reset_index() \n",
        "\n",
        "    ### Step 2: add min and max columns\n",
        "    # 1252 rows or events present in filtered events\n",
        "    df_maxs = counts.groupby(['feature_id'], as_index=True).max() \\\n",
        "      .rename(columns={'freq':'freq_max', 'pid':'pid_max'})\n",
        "\n",
        "    # leftouter join to attach max values\n",
        "    # 154754 rows preserved\n",
        "    inner_join2 = pd.merge(left=counts,\n",
        "                          right=df_maxs,\n",
        "                          on=['feature_id'],\n",
        "                          how = 'inner',\n",
        "                          suffixes=('_orig', '_max')).rename(columns={'freq':'freq_orig'})\n",
        "\n",
        "    ### Step 3: add new column- normalize against max AND min values\n",
        "    freq_min = 0\n",
        "    inner_join2['feature_value'] = (inner_join2['freq_orig'] - freq_min) / (inner_join2['freq_max'] - freq_min)\n",
        "\n",
        "    inner_join2 = inner_join2.fillna(0)\n",
        "\n",
        "    #cleanup\n",
        "    aggregated_events = inner_join2 \\\n",
        "      .drop(columns=['freq_max','freq_orig', 'pid_max','event_id_orig', 'event_id_max'])\n",
        "\n",
        "    return aggregated_events"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "57aa824b8fd9cf75ae7da47f0ff3156b",
          "grade": false,
          "grade_id": "cell-8db3e08c181b90cc",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "EaUepqRLISLK"
      },
      "source": [
        "### 2.4 Convert to SVMLight format\n",
        "\n",
        "If we have a lot of features but not many hits (i.e. it has only a few nonzero elements), its best to compress the space.\n",
        "\n",
        "```\n",
        "<line> .=. <target> <feature>:<value> <feature>:<value>\n",
        "<target> .=. 1 | 0\n",
        "<feature> .=. <integer>\n",
        "<value> .=. <float>\n",
        "```\n",
        "\n",
        "Feature/value pairs MUST be ranked by ascending feature number. The SVMLight format looks like: \n",
        "\n",
        "```\n",
        "1 2:0.5 3:0.12 10:0.9 2000:0.3\n",
        "0 4:1.0 78:0.6 1009:0.2\n",
        "1 33:0.1 34:0.98 1000:0.8 3300:0.2\n",
        "1 34:0.1 389:0.32\n",
        "```\n",
        "\n",
        "where each row is a patient, label 1 indicates heart failure and feature-value pairs follow **sorted** by the feature index (idx) value."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_svmlight_file\n",
        "\n",
        "def bag_to_svmlight(input):\n",
        "    return ' '.join(( \"%d:%f\" % (fid, float(fvalue)) for fid, fvalue in input))\n",
        "\n",
        "#input: features and label stored in the svmlight_file\n",
        "#output: X_train, Y_train\n",
        "#Note: hardcode the number of features\n",
        "def get_data_from_svmlight(svmlight_file):\n",
        "    data_train = load_svmlight_file(svmlight_file,n_features=1473)\n",
        "    X_train = data_train[0]\n",
        "    Y_train = data_train[1]\n",
        "    return X_train, Y_train"
      ],
      "metadata": {
        "id": "pqn-eNR11h_U"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T04:30:03.114846Z",
          "start_time": "2022-01-17T04:30:02.791439Z"
        },
        "deletable": false,
        "id": "WqUMho00ISLK"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "def create_features(events_in, hf_in, feature_map_in):\n",
        "    '''\n",
        "    Returns two dicts:\n",
        "    1. patient_features: Key is pid and value is array of tuples(feature_id, feature_value). \n",
        "    2. hf: Key is pid and value is heart failure label.\n",
        "\n",
        "    indx_vid > index, pid, indx_vid\n",
        "    events_in > pid, event_id, vid, value\n",
        "    hf_in > 'pid', 'vid', 'label'\n",
        "    feature_map_in > 'idx' , 'event_id'\n",
        "    filtered_events > 'pid', 'event_id', 'value'\n",
        "    aggregated_events > ['pid', 'feature_id', 'feature_value']\n",
        "    '''\n",
        "\n",
        "    indx_vid = calculate_index_vid(events_in, hf_in)\n",
        "\n",
        "    #Filter events in the observation window\n",
        "    filtered_events = filter_events(events_in, indx_vid)\n",
        "\n",
        "    #Aggregate the event values for each patient \n",
        "    aggregated_events = aggregate_events(filtered_events, _, feature_map_in)\n",
        "\n",
        "    patient_features = None\n",
        "    hf = None\n",
        "  \n",
        "    # turn the dataframes into dicts\n",
        "    aggregated_events = pd.DataFrame(aggregated_events.set_index('pid').apply(tuple, axis=1)).sort_values(by=0,ascending=True)\n",
        "    patient_features = aggregated_events.stack().groupby(level=0).apply(list).to_dict()\n",
        "\n",
        "    hf = hf_in\n",
        "    hf = hf.drop(columns=['vid']).groupby('pid').count()\n",
        "    hf = hf.sort_index().stack().groupby(level=0).apply(int).to_dict()\n",
        "  \n",
        "    return patient_features, hf\n",
        "\n",
        "def save_svmlight(patient_features, hf, op_file):\n",
        "    '''\n",
        "    Creates op_file in svmlight format\n",
        "    '''\n",
        "    \n",
        "    deliverable = open(op_file, 'wb')\n",
        "\n",
        "    # Create list of strings\n",
        "    l = [( str(hf[pid] if pid in hf else '0') + ' ' + bag_to_svmlight(tuples)) for pid, tuples in patient_features.items()]\n",
        "    # write to file\n",
        "    for example in l:\n",
        "      label = example.split(\" \", 1)[0]\n",
        "      features = example.split(\" \", 1)[1]\n",
        "      deliverable.write(bytes(f\"{label} {features} \\n\", 'utf-8'))\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T04:30:18.478851Z",
          "start_time": "2022-01-17T04:30:03.320257Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d511eacce421e41b3f8e0a5d0e69639a",
          "grade": false,
          "grade_id": "cell-5be75ccfce6fe003",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "22nRWKZxISLL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "426d4a81-c2f6-403d-de65-f598d41855cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:82: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    events_in, hf_in, feature_map_in = read_csv(TRAIN_DATA_PATH)\n",
        "    patient_features, hf = create_features(events_in, hf_in, feature_map_in)\n",
        "    save_svmlight(patient_features, hf, 'features_svmlight.train')\n",
        "    \n",
        "    events_in, hf_in, feature_map_in = read_csv(VAL_DATA_PATH)\n",
        "    patient_features, hf = create_features(events_in, hf_in, feature_map_in)\n",
        "    save_svmlight(patient_features, hf, 'features_svmlight.val')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "c28533d723309779b8530cb354815f28",
          "grade": false,
          "grade_id": "cell-a02bc53e8da63b0b",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "_o4fo7F8ISLL"
      },
      "source": [
        "## 3 Predictive Modeling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "94422b239845574543cc7842983e0b86",
          "grade": false,
          "grade_id": "cell-87c0e61ad837e42c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "vDLIkw9TISLL"
      },
      "source": [
        "### 3.1 Model Build\n",
        "\n",
        "Here I implement Logistic Regression, SVM and a Decision Tree with sklearn and evaluate on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T04:30:18.840336Z",
          "start_time": "2022-01-17T04:30:18.480730Z"
        },
        "deletable": false,
        "id": "ohjNgEB9ISLL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1bcc72c-7827-4d2f-84ff-afd790edb1de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "______________________________________________\n",
            "Classifier: Logistic Regression\n",
            "Accuracy: 0.856338028169014\n",
            "Precision: 0.8357933579335793\n",
            "Recall: 0.937888198757764\n",
            "F1-score: 0.8839024390243903\n",
            "______________________________________________\n",
            "\n",
            "______________________________________________\n",
            "Classifier: SVM\n",
            "Accuracy: 0.9070422535211268\n",
            "Precision: 0.896484375\n",
            "Recall: 0.9503105590062112\n",
            "F1-score: 0.9226130653266331\n",
            "______________________________________________\n",
            "\n",
            "______________________________________________\n",
            "Classifier: Decision Tree\n",
            "Accuracy: 0.703420523138833\n",
            "Precision: 0.6657355679702048\n",
            "Recall: 0.9868875086266391\n",
            "F1-score: 0.7951070336391437\n",
            "______________________________________________\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_svmlight_file\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import *\n",
        "\n",
        "\n",
        "RANDOM_STATE = 545510477\n",
        "\n",
        "\n",
        "def logistic_regression_pred(X_train, Y_train):\n",
        "    \"\"\"\n",
        "    Returns predictions with a binary logistic classifier\n",
        "    \"\"\"\n",
        "\n",
        "    clfr = LogisticRegression(random_state=RANDOM_STATE)\n",
        "    clfr.fit(X=X_train, y=Y_train)\n",
        "    Y_pred = clfr.predict(X_train)\n",
        "    return Y_pred\n",
        "\n",
        "  \n",
        "def svm_pred(X_train, Y_train):\n",
        "    \"\"\"\n",
        "    Returns predictions with an SVM classifier\n",
        "    \"\"\"\n",
        "\n",
        "    clfr = LinearSVC(random_state=RANDOM_STATE)\n",
        "    clfr.fit(X=X_train, y=Y_train)\n",
        "    Y_pred = clfr.predict(X_train)\n",
        "    return Y_pred\n",
        "\n",
        "    \n",
        "def decisionTree_pred(X_train, Y_train):\n",
        "    \"\"\"\n",
        "    Returns predictions with a decision tree of max_depth 5\n",
        "    \"\"\"\n",
        "\n",
        "    clfr = DecisionTreeClassifier(random_state=RANDOM_STATE, max_depth=5)\n",
        "    clfr.fit(X=X_train, y=Y_train)\n",
        "    Y_pred = clfr.predict(X_train)\n",
        "    return Y_pred\n",
        "\n",
        "    \n",
        "\n",
        "def classification_metrics(Y_pred, Y_true):\n",
        "    \"\"\"\n",
        "    input: Y_pred, Y_true\n",
        "    output: accuracy, precision, recall, f1-score\n",
        "    \"\"\"\n",
        "    \n",
        "    # %age of correct Y_pred (1 or 0) over numrows\n",
        "    acc = accuracy_score(y_true=Y_true, y_pred=Y_pred)\n",
        "    # count of correct (Y_pred == 1) over len(Y_pred)\n",
        "    precision = precision_score(y_true=Y_true, y_pred=Y_pred)\n",
        "    # count of correct (Y_pred == 1) over that plus incorrect (Y_pred == 0)\n",
        "    recall = recall_score(y_true=Y_true, y_pred=Y_pred)\n",
        "    # harmonic mean of precision and recall n / (1/x) * (1/y)\n",
        "    f1score = f1_score(y_true=Y_true, y_pred=Y_pred)\n",
        "\n",
        "    return acc, precision, recall, f1score\n",
        "\n",
        "    \n",
        "def display_metrics(classifierName, Y_pred, Y_true):\n",
        "    print(\"______________________________________________\")\n",
        "    print((\"Classifier: \"+classifierName))\n",
        "    acc, precision, recall, f1score = classification_metrics(Y_pred,Y_true)\n",
        "    print((\"Accuracy: \"+str(acc)))\n",
        "    print((\"Precision: \"+str(precision)))\n",
        "    print((\"Recall: \"+str(recall)))\n",
        "    print((\"F1-score: \"+str(f1score)))\n",
        "    print(\"______________________________________________\")\n",
        "    print(\"\")\n",
        "\n",
        "    \n",
        "if __name__ == '__main__': \n",
        "    X_train, Y_train = get_data_from_svmlight(\"features_svmlight.train\")\n",
        "\n",
        "    display_metrics(\"Logistic Regression\", logistic_regression_pred(X_train, Y_train), Y_train)\n",
        "    display_metrics(\"SVM\",svm_pred(X_train, Y_train),Y_train)\n",
        "    display_metrics(\"Decision Tree\", decisionTree_pred(X_train, Y_train), Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T04:30:19.404126Z",
          "start_time": "2022-01-17T04:30:19.123750Z"
        },
        "deletable": false,
        "id": "A8sMT9KJISLM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc1d3af6-4c60-438b-dd71-465f9dc69061"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "______________________________________________\n",
            "Classifier: Logistic Regression\n",
            "Accuracy: 0.6937086092715232\n",
            "Precision: 0.7345360824742269\n",
            "Recall: 0.776566757493188\n",
            "F1-score: 0.7549668874172186\n",
            "______________________________________________\n",
            "\n",
            "______________________________________________\n",
            "Classifier: SVM\n",
            "Accuracy: 0.640728476821192\n",
            "Precision: 0.7038043478260869\n",
            "Recall: 0.7057220708446866\n",
            "F1-score: 0.7047619047619047\n",
            "______________________________________________\n",
            "\n",
            "______________________________________________\n",
            "Classifier: Decision Tree\n",
            "Accuracy: 0.6821192052980133\n",
            "Precision: 0.6611418047882136\n",
            "Recall: 0.9782016348773842\n",
            "F1-score: 0.789010989010989\n",
            "______________________________________________\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_svmlight_file\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import *\n",
        "\n",
        "\n",
        "RANDOM_STATE = 545510477\n",
        "\n",
        "\n",
        "def logistic_regression_pred(X_train, Y_train, X_test):\n",
        "    \"\"\"\n",
        "    Returns predictions from a logistic regression classifier on the test set \n",
        "    \n",
        "    input: X_train, Y_train and X_test\n",
        "    output: Y_pred\n",
        "    \"\"\"\n",
        "    \n",
        "    clfr = LogisticRegression(random_state=RANDOM_STATE)\n",
        "    clfr.fit(X=X_train, y=Y_train)\n",
        "    Y_pred = clfr.predict(X_test)\n",
        "    return Y_pred\n",
        "    \n",
        "\n",
        "\n",
        "def svm_pred(X_train, Y_train, X_test):    \n",
        "    \"\"\"\n",
        "    Returns predictions from an SVM classifier on the test set \n",
        "    \n",
        "    input: X_train, Y_train and X_test\n",
        "    output: Y_pred\n",
        "    \"\"\"\n",
        "    \n",
        "    clfr = LinearSVC(random_state=RANDOM_STATE)\n",
        "    clfr.fit(X=X_train, y=Y_train)\n",
        "    Y_pred = clfr.predict(X_test)\n",
        "    return Y_pred\n",
        "\n",
        "    \n",
        "#input: X_train, Y_train and X_test\n",
        "#output: Y_pred\n",
        "def decisionTree_pred(X_train, Y_train, X_test):\n",
        "    \"\"\"\n",
        "    Returns predictions from a decision tree classifier on the test set \n",
        "    \n",
        "    input: X_train, Y_train and X_test\n",
        "    output: Y_pred\n",
        "    \"\"\"\n",
        "    \n",
        "    clfr = DecisionTreeClassifier(random_state=RANDOM_STATE, max_depth=5)\n",
        "    clfr.fit(X=X_train, y=Y_train)\n",
        "    Y_pred = clfr.predict(X_test)\n",
        "    return Y_pred\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def classification_metrics(Y_pred, Y_true):  \n",
        "    \"\"\"\n",
        "    Returns metrics.\n",
        "\n",
        "    #input: Y_pred,Y_true\n",
        "    #output: accuracy, precision, recall, f1-score\n",
        "    \"\"\"\n",
        "    \n",
        "    # %age of correct Y_pred (1 or 0) over numrows\n",
        "    acc = accuracy_score(y_true=Y_true, y_pred=Y_pred)\n",
        "    # count of correct (Y_pred == 1) over len(Y_pred)\n",
        "    precision = precision_score(y_true=Y_true, y_pred=Y_pred)\n",
        "    # count of correct (Y_pred == 1) over that plus incorrect (Y_pred == 0)\n",
        "    recall = recall_score(y_true=Y_true, y_pred=Y_pred)\n",
        "    # harmonic mean of precision and recall n / (1/x) * (1/y)\n",
        "    f1score = f1_score(y_true=Y_true, y_pred=Y_pred)\n",
        "\n",
        "    return acc, precision, recall, f1score\n",
        "    \n",
        "    \n",
        "def display_metrics(classifierName, Y_pred, Y_true):\n",
        "    print(\"______________________________________________\")\n",
        "    print((\"Classifier: \"+classifierName))\n",
        "    acc, precision, recall, f1score = classification_metrics(Y_pred,Y_true)\n",
        "    print((\"Accuracy: \"+str(acc)))\n",
        "    print((\"Precision: \"+str(precision)))\n",
        "    print((\"Recall: \"+str(recall)))\n",
        "    print((\"F1-score: \"+str(f1score)))\n",
        "    print(\"______________________________________________\")\n",
        "    print(\"\")\n",
        "\n",
        "    \n",
        "if __name__ == '__main__': \n",
        "    X_train, Y_train = get_data_from_svmlight(\"features_svmlight.train\")\n",
        "    X_test, Y_test = get_data_from_svmlight(os.path.join(\"features_svmlight.val\"))\n",
        "\n",
        "    display_metrics(\"Logistic Regression\", logistic_regression_pred(X_train, Y_train, X_test), Y_test)\n",
        "    display_metrics(\"SVM\", svm_pred(X_train, Y_train, X_test), Y_test)\n",
        "    display_metrics(\"Decision Tree\", decisionTree_pred(X_train, Y_train, X_test), Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "f417499e53d12d286cd07c58cb33130e",
          "grade": false,
          "grade_id": "cell-1a6ab34b3605550e",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "FYcIAOERISLM"
      },
      "source": [
        "### 3.2 Model Validation\n",
        "\n",
        "Here I implement cross-validation:\n",
        "\n",
        "- K-fold: Divide all the data into $k$ groups of samples. Each time $\\frac{1}{k}$ samples will be used as test data and the remaining samples as training data.\n",
        "- Randomized K-fold: Iteratively random shuffle the whole dataset and use top specific percentage of data (I use 20%) as training and the rest as test. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T04:30:20.379732Z",
          "start_time": "2022-01-17T04:30:19.679182Z"
        },
        "deletable": false,
        "id": "ji0et-HaISLM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1794a19-e544-4484-9374-d38a35fc1307"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifier: SVD\n",
            "Average F1 Score in KFold CV: 0.7258461959533061\n",
            "Average F1 Score in Randomised CV: 0.7195678940019832\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import KFold, ShuffleSplit\n",
        "from numpy import mean\n",
        "\n",
        "\n",
        "RANDOM_STATE = 545510477\n",
        "\n",
        "\n",
        "def get_f1_kfold(X, Y, k=5):\n",
        "    \"\"\"\n",
        "    Returns mean f1 score of all the fold runs. Repeats the train/test process k times.\n",
        "\n",
        "    Input: test and train row indices for each run\n",
        "    \"\"\"\n",
        "    \n",
        "    acc = []\n",
        "    f1score = []\n",
        "    # create the index mask\n",
        "    for i, (train_index, test_index) in enumerate(KFold(n_splits=k).split(X, Y)):\n",
        "      X_train, Y_train = X[train_index], Y[train_index]\n",
        "      X_test, Y_test = X[test_index], Y[test_index]\n",
        "      clfr = LinearSVC(random_state=RANDOM_STATE)\n",
        "      clfr.fit(X=X_train, y=Y_train)\n",
        "      Y_pred = clfr.predict(X_test)\n",
        "      acc.append(accuracy_score(y_true=Y_test, y_pred=Y_pred))\n",
        "      f1score.append(f1_score(y_true=Y_test, y_pred=Y_pred))\n",
        "      # print(f'iteration: {i} accuracy {acc[i]} f1score {f1score[i]}')\n",
        "\n",
        "    return np.mean(np.array(f1score)) #, np.mean(np.array(acc))\n",
        "\n",
        "    \n",
        "def get_f1_randomisedCV(X, Y, iterNo=5, test_percent=0.20):\n",
        "    \"\"\"\n",
        "    Returns mean f1 score of all the fold runs. Repeats the train/test process iterNo times.\n",
        "\n",
        "    Input: test and train row indices for each run\n",
        "    \"\"\"\n",
        "\n",
        "    acc = []\n",
        "    f1score = []\n",
        "    # create the index mask\n",
        "    for i, (train_index, test_index) in enumerate(ShuffleSplit(n_splits=iterNo, test_size=test_percent, random_state=RANDOM_STATE).split(X, Y)):\n",
        "      X_train, Y_train = X[train_index], Y[train_index]\n",
        "      X_test, Y_test = X[test_index], Y[test_index]\n",
        "      clfr = LinearSVC(random_state=RANDOM_STATE)\n",
        "      clfr.fit(X=X_train, y=Y_train)\n",
        "      Y_pred = clfr.predict(X_test)\n",
        "      acc.append(accuracy_score(y_true=Y_test, y_pred=Y_pred))\n",
        "      f1score.append(f1_score(y_true=Y_test, y_pred=Y_pred))\n",
        "      # print(f'iteration: {i} accuracy {acc[i]} f1score {f1score[i]}')\n",
        "      \n",
        "    return np.mean(np.array(f1score)) #, np.mean(np.array(acc))\n",
        "\n",
        "    \n",
        "if __name__ == '__main__': \n",
        "    X,Y = get_data_from_svmlight(\"features_svmlight.train\")\n",
        "    print(\"Classifier: SVD\")\n",
        "    f1_k = get_f1_kfold(X,Y)\n",
        "    print((\"Average F1 Score in KFold CV: \"+str(f1_k)))\n",
        "    f1_r = get_f1_randomisedCV(X,Y)\n",
        "    print((\"Average F1 Score in Randomised CV: \"+str(f1_r)))"
      ]
    }
  ],
  "metadata": {
    "illinois_payload": {
      "b64z": "",
      "nb_path": "release/HW1/HW1.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3 (Threads: 2)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "832px",
        "left": "419px",
        "top": "110px",
        "width": "311.390625px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "HW1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}